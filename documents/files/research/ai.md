# Different AIs for Stratego

## ğŸ² 1. **FAFO (Random)**

**Idea:**
Moves are chosen randomly from all legal actions.

**âœ… Pros:**

* Easiest to implement.
* Provides a good baseline for testing.
* Always unpredictable.

**âŒ Cons:**

* No learning or strategy.
* Performs very poorly against anything smarter.

**ğŸ§  Trainable?**
Not really â€” random agents donâ€™t learn. They can, however, be used as â€œopponentsâ€ for training smarter agents (e.g., reinforcement learning warm-up).

---

## âš™ï¸ 2. **Heuristic**

**Idea:**
Uses manually designed rules or evaluations (e.g., value pieces, favor capturing lower-ranked enemies, protect flag).

**âœ… Pros:**

* Fast and simple.
* Encodes expert knowledge easily.
* Can perform decently without heavy computation.

**âŒ Cons:**

* Limited by human bias or oversimplified rules.
* Hard to adapt to hidden information and deception in Stratego.
* Doesnâ€™t improve without manual tuning.

**ğŸ§  Trainable?**
Semi-trainable â€” you can optimize weights of heuristics (e.g., via genetic algorithms or reinforcement learning) to improve over time.

---

## â™Ÿï¸ 3. **Minimax**

**Idea:**
Explores the game tree assuming both players play optimally. Each node alternates between maximizing and minimizing the evaluation score.

**âœ… Pros:**

* Theoretically strong â€” finds optimal play if the tree is fully explored.
* Great for deterministic perfect-information games (like chess).

**âŒ Cons:**

* Stratego has **hidden information** (unknown opponent pieces), so minimax canâ€™t model uncertainty well.
* The branching factor is huge â†’ needs pruning (Î±â€“Î² pruning) and depth limits.
* Struggles when bluffing or incomplete knowledge is key.

**ğŸ§  Trainable?**
Partly â€” you can train the **evaluation function** (e.g., using self-play to learn board value estimates). But minimaxâ€™s structure itself is not learnable.

---

## ğŸŒ³ 4. **MCTS (Monte Carlo Tree Search)**

**Idea:**
Simulates many random playouts from the current position to estimate move quality statistically. Expands the tree towards promising moves using exploration/exploitation balance.

**âœ… Pros:**

* Handles huge and uncertain state spaces better than minimax.
* Adapts dynamically â€” no fixed evaluation needed.
* Excellent for **hidden-information games** (if you include belief modeling).
* Basis of AlphaZero-style learning.

**âŒ Cons:**

* Computationally heavy (many simulations).
* Quality depends on playout policy (random = weak, learned = stronger).
* Requires many iterations for stable results.

**ğŸ§  Trainable?**
Yes â€” very trainable. You can:

* Train a **policy network** to guide simulations.
* Train a **value network** to replace random rollouts.
* Use **self-play reinforcement learning** (AlphaZero-style) to improve both.

---

## âš”ï¸ TL;DR â€” in Stratego context

| Algorithm | Info type     | Strength         | Weakness             | Trainable  | Notes                 |
| --------- | ------------- | ---------------- | -------------------- | ---------- | --------------------- |
| Random    | None          | Unpredictable    | Dumb                 | âŒ          | Use for testing       |
| Heuristic | Expert rules  | Fast             | Rigid, biased        | âš™ï¸ Partial | Good baseline         |
| Minimax   | Deterministic | Strategic        | Hidden info kills it | âš™ï¸ Partial | Needs belief modeling |
| MCTS      | Statistical   | Flexible, strong | Heavy compute        | âœ… Full     | Best long-term option |

## Expected runtime difference between FAFO & MCTS

* **Baseline:** 1.44 s for **1000 games total** (so **0.00144 s per game**).
* **moves/game** = total moves (both players combined). I show 50, 100, 200.
* Random agent = 1 â€œevalâ€ per move; MCTS uses **S** simulations per decision.
* One MCTS vs random â†’ MCTS player makes about half the decisions.

### Key results (rounded)

For **S = 100 sims / decision**

* **One MCTS vs random:** **72.72 s** total for 1000 games â‰ˆ **1.21 min** (â‰ˆ **50.5Ã—** slower than baseline).
* **Both MCTS:** **144.0 s** â‰ˆ **2.40 min** (â‰ˆ **100Ã—** slower).

For **S = 1,000**

* **One MCTS:** **720.72 s** â‰ˆ **12.01 min** (â‰ˆ **500.5Ã—**).
* **Both MCTS:** **1,440 s** = **24.00 min** (â‰ˆ **1000Ã—**).

For **S = 10,000**

* **One MCTS:** **7,200.72 s** â‰ˆ **2.00 hours** (â‰ˆ **5000.5Ã—**).
* **Both MCTS:** **14,400 s** = **4.00 hours** (â‰ˆ **10000Ã—**).

**TL;DR:** Even with the tiny baseline (1.44 s / 1000 games), MCTS blows up quickly. Cost scales roughly **linearly** with simulations (S) and decisions per game.
**If runtime matters:** lower S, parallelize sims, use a learned policy/value to cut sims, or only use MCTS on high-value turns.
