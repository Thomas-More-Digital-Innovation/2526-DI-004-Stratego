# Different AIs for Stratego

## ğŸ² 1. **Random**

**Idea:**
Moves are chosen randomly from all legal actions.

**âœ… Pros:**

* Easiest to implement.
* Provides a good baseline for testing.
* Always unpredictable.

**âŒ Cons:**

* No learning or strategy.
* Performs very poorly against anything smarter.

**ğŸ§  Trainable?**
Not really â€” random agents donâ€™t learn. They can, however, be used as â€œopponentsâ€ for training smarter agents (e.g., reinforcement learning warm-up).

---

## âš™ï¸ 2. **Heuristic**

**Idea:**
Uses manually designed rules or evaluations (e.g., value pieces, favor capturing lower-ranked enemies, protect flag).

**âœ… Pros:**

* Fast and simple.
* Encodes expert knowledge easily.
* Can perform decently without heavy computation.

**âŒ Cons:**

* Limited by human bias or oversimplified rules.
* Hard to adapt to hidden information and deception in Stratego.
* Doesnâ€™t improve without manual tuning.

**ğŸ§  Trainable?**
Semi-trainable â€” you can optimize weights of heuristics (e.g., via genetic algorithms or reinforcement learning) to improve over time.

---

## â™Ÿï¸ 3. **Minimax**

**Idea:**
Explores the game tree assuming both players play optimally. Each node alternates between maximizing and minimizing the evaluation score.

**âœ… Pros:**

* Theoretically strong â€” finds optimal play if the tree is fully explored.
* Great for deterministic perfect-information games (like chess).

**âŒ Cons:**

* Stratego has **hidden information** (unknown opponent pieces), so minimax canâ€™t model uncertainty well.
* The branching factor is huge â†’ needs pruning (Î±â€“Î² pruning) and depth limits.
* Struggles when bluffing or incomplete knowledge is key.

**ğŸ§  Trainable?**
Partly â€” you can train the **evaluation function** (e.g., using self-play to learn board value estimates). But minimaxâ€™s structure itself is not learnable.

---

## ğŸŒ³ 4. **MCTS (Monte Carlo Tree Search)**

**Idea:**
Simulates many random playouts from the current position to estimate move quality statistically. Expands the tree towards promising moves using exploration/exploitation balance.

**âœ… Pros:**

* Handles huge and uncertain state spaces better than minimax.
* Adapts dynamically â€” no fixed evaluation needed.
* Excellent for **hidden-information games** (if you include belief modeling).
* Basis of AlphaZero-style learning.

**âŒ Cons:**

* Computationally heavy (many simulations).
* Quality depends on playout policy (random = weak, learned = stronger).
* Requires many iterations for stable results.

**ğŸ§  Trainable?**
Yes â€” very trainable. You can:

* Train a **policy network** to guide simulations.
* Train a **value network** to replace random rollouts.
* Use **self-play reinforcement learning** (AlphaZero-style) to improve both.

---

## âš”ï¸ TL;DR â€” in Stratego context

| Algorithm | Info type     | Strength         | Weakness             | Trainable  | Notes                 |
| --------- | ------------- | ---------------- | -------------------- | ---------- | --------------------- |
| Random    | None          | Unpredictable    | Dumb                 | âŒ          | Use for testing       |
| Heuristic | Expert rules  | Fast             | Rigid, biased        | âš™ï¸ Partial | Good baseline         |
| Minimax   | Deterministic | Strategic        | Hidden info kills it | âš™ï¸ Partial | Needs belief modeling |
| MCTS      | Statistical   | Flexible, strong | Heavy compute        | âœ… Full     | Best long-term option |
